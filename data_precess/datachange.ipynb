{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a86b2d-98bb-4d5c-a261-8faa15a8bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "临时文件路径: /tmp/tmp2ay_gh73.ts\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "\n",
    "# 创建临时TS文件\n",
    "def create_mock_tsfile():\n",
    "    content = \"\"\"@problemName MockMultivariate\n",
    "@timestamps false\n",
    "@univariate false\n",
    "@equalLength true\n",
    "@dimensions 3\n",
    "@seriesLength 4\n",
    "@classLabel true 0 1\n",
    "@data\n",
    "(1.1,2.1,3.1,4.1),(1.2,2.2,3.2,4.2),(1.3,2.3,3.3,4.3):0\n",
    "(5.1,6.1,7.1,8.1),(5.2,6.2,7.2,8.2),(5.3,6.3,7.3,8.3):1\"\"\"\n",
    "    \n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.ts')\n",
    "    with open(tmp.name, 'w') as f:\n",
    "        f.write(content)\n",
    "    return tmp.name\n",
    "\n",
    "filepath = create_mock_tsfile()\n",
    "print(f\"临时文件路径: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41aed51-0686-4df4-b5a3-400a6148a88c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '(1.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39312/1975603879.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 加载数据并检查结构\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_tsfile_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== 解析结果验证 ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"数据形状: {X.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 应输出 (2, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sktime/datasets/_data_io.py\u001b[0m in \u001b[0;36mload_from_tsfile_to_dataframe\u001b[0;34m(full_file_path_and_name, return_separate_X_and_y, replace_missing_vals_with)\u001b[0m\n\u001b[1;32m    995\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                                 \u001b[0mdata_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                                 \u001b[0mdata_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_series\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m                                 \u001b[0minstance_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sktime/datasets/_data_io.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    995\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                                 \u001b[0mdata_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                                 \u001b[0mdata_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_series\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m                                 \u001b[0minstance_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '(1.1'"
     ]
    }
   ],
   "source": [
    "# 加载数据并检查结构\n",
    "X, y = load_from_tsfile_to_dataframe(filepath)\n",
    "\n",
    "print(\"\\n=== 解析结果验证 ===\")\n",
    "print(f\"数据形状: {X.shape}\")  # 应输出 (2, 3)\n",
    "print(f\"标签: {y}\")           # 应输出 [0, 1]\n",
    "\n",
    "# 检查第一个样本\n",
    "print(\"\\n第一个样本:\")\n",
    "print(f\"通道1: {X.iloc[0, 0].values}\")  # 应输出 [1.1, 2.1, 3.1, 4.1]\n",
    "print(f\"通道2: {X.iloc[0, 1].values}\")  # 应输出 [1.2, 2.2, 3.2, 4.2]\n",
    "print(f\"通道3: {X.iloc[0, 2].values}\")  # 应输出 [1.3, 2.3, 3.3, 4.3]\n",
    "\n",
    "# 检查DataFrame类型\n",
    "print(\"\\n数据类型验证:\")\n",
    "print(f\"单元格类型: {type(X.iloc[0,0])}\")  # 应输出 pandas.Series\n",
    "print(f\"通道数: {X.shape[1]}\")          # 应等于@dimensions声明的3\n",
    "print(f\"时间步长: {len(X.iloc[0,0])}\")  # 应等于@seriesLength声明的4\n",
    "\n",
    "# 清理临时文件\n",
    "os.unlink(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1706af2-069b-460b-a6f0-94697646911d",
   "metadata": {},
   "source": [
    "10.22问题记录：\n",
    "原始模型的多通道貌似是用括号分开？虽然输出看没有括号像是展平 但是真直接展平  模型输入尺寸就不对；所以应该是括号分开，但是函数load_from_tsfile_to_dataframe又没法识别括号 这样没法转化数据类型 就一直报错  could not convert string to float: '(1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba391f7-129c-4257-be58-758fc3bb45fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件路径: /tmp/tmp9p0s0bem.ts\n",
      "文件内容:\n",
      "@problemName ProperLabelDemo\n",
      "@timestamps false\n",
      "@missing false\n",
      "@univariate false\n",
      "@equalLength true\n",
      "@dimensions 2\n",
      "@seriesLength 3\n",
      "@classLabel true 0 1 2\n",
      "@data\n",
      "1.1,1.2,1.3:2.1,2.2,2.3:0\n",
      "4.1,4.2,4.3:5.1,5.2,5.3:1\n",
      "7.1,7.2,7.3:8.1,8.2,8.3:2\n",
      "\n",
      "=== 解析成功 ===\n",
      "数据形状: (3, 2)\n",
      "标签: ['0' '1' '2']\n",
      "\n",
      "第一个样本:\n",
      "通道1: [1.1, 1.2, 1.3]\n",
      "通道2: [2.1, 2.2, 2.3]\n",
      "对应标签: 0\n"
     ]
    }
   ],
   "source": [
    "##尝试冒号分割\n",
    "import tempfile\n",
    "import os\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "\n",
    "def create_proper_tsfile():\n",
    "    \"\"\"创建符合sktime完整要求的TS文件\"\"\"\n",
    "    content = \"\"\"@problemName ProperLabelDemo\n",
    "@timestamps false\n",
    "@missing false\n",
    "@univariate false\n",
    "@equalLength true\n",
    "@dimensions 2\n",
    "@seriesLength 3\n",
    "@classLabel true 0 1 2\n",
    "@data\n",
    "1.1,1.2,1.3:2.1,2.2,2.3:0\n",
    "4.1,4.2,4.3:5.1,5.2,5.3:1\n",
    "7.1,7.2,7.3:8.1,8.2,8.3:2\"\"\"\n",
    "    \n",
    "    tmp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.ts')\n",
    "    tmp.write(content)\n",
    "    tmp.close()\n",
    "    return tmp.name\n",
    "\n",
    "# 测试加载\n",
    "filepath = create_proper_tsfile()\n",
    "print(f\"文件路径: {filepath}\")\n",
    "print(\"文件内容:\")\n",
    "with open(filepath) as f:\n",
    "    print(f.read())\n",
    "\n",
    "try:\n",
    "    X, y = load_from_tsfile_to_dataframe(filepath)\n",
    "    \n",
    "    print(\"\\n=== 解析成功 ===\")\n",
    "    print(f\"数据形状: {X.shape}\")  # 应输出 (3, 2)\n",
    "    print(f\"标签: {y}\")          # 应输出 [0 1 2]\n",
    "    \n",
    "    print(\"\\n第一个样本:\")\n",
    "    print(f\"通道1: {X.iloc[0, 0].values.tolist()}\")\n",
    "    print(f\"通道2: {X.iloc[0, 1].values.tolist()}\")\n",
    "    print(f\"对应标签: {y[0]}\")\n",
    "    \n",
    "finally:\n",
    "    os.unlink(filepath)  # 清理临时文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce545d26-8abf-4e8e-a22a-ff3776206c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已加载被试 chb01.npz 的数据\n",
      "✅ 已加载被试 chb02.npz 的数据\n",
      "✅ 已加载被试 chb03.npz 的数据\n",
      "✅ 已加载被试 chb05.npz 的数据\n",
      "✅ 已加载被试 chb09.npz 的数据\n",
      "✅ 已加载被试 chb10.npz 的数据\n",
      "✅ 已加载被试 chb13.npz 的数据\n",
      "✅ 已加载被试 chb14.npz 的数据\n",
      "✅ 已加载被试 chb18.npz 的数据\n",
      "✅ 已加载被试 chb19.npz 的数据\n",
      "✅ 已加载被试 chb20.npz 的数据\n",
      "✅ 已加载被试 chb21.npz 的数据\n",
      "✅ 已加载被试 chb23.npz 的数据\n",
      "\n",
      "=== 数据集划分 ===\n",
      "总被试数: 13\n",
      "训练集被试 (10): chb18.npz, chb10.npz, chb03.npz, chb02.npz, chb23.npz, chb09.npz, chb14.npz, chb20.npz, chb05.npz, chb13.npz\n",
      "测试集被试 (3): chb21.npz, chb19.npz, chb01.npz\n",
      "\n",
      "=== 保存文件中 ===\n",
      "\n",
      "=== 转换完成 ===\n",
      "输出文件:\n",
      "/root/autodl-tmp/Time-Series-Library/dataset/\n",
      "├── CHBMIT_EEG_BySubject_TRAIN.ts\n",
      "├── CHBMIT_EEG_BySubject_TEST.ts\n",
      "└── CHBMIT_EEG_BySubject_METADATA.txt\n",
      "\n",
      "数据集统计:\n",
      "- 训练集: 2629 样本 (来自 10 个被试)\n",
      "- 测试集: 378 样本 (来自 3 个被试)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_npz_to_ts_by_subject(npz_files, output_dir, dataset_name=\"CHBMIT_EEG\", test_subjects=3, random_state=42):\n",
    "    \"\"\"\n",
    "    按被试划分训练/测试集的TS转换器\n",
    "    参数：\n",
    "        test_subjects: 测试集被试数量（不是比例）\n",
    "    \"\"\"\n",
    "    # 初始化\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # === 第一步：按被试加载数据 ===\n",
    "    subject_data = defaultdict(list)\n",
    "    subject_ids = set()\n",
    "    \n",
    "    for npz_file in sorted(npz_files):\n",
    "        try:\n",
    "            with np.load(npz_file, allow_pickle=True) as data:\n",
    "                subject_id = os.path.basename(npz_file).split('_')[1]  # 提取被试ID（如chb01）\n",
    "                subject_ids.add(subject_id)\n",
    "                windows = np.nan_to_num(data['windows'], nan=0.0)\n",
    "                labels = data['labels'].astype(int)\n",
    "                subject_data[subject_id].append({\n",
    "                    'windows': windows.astype(np.float32),\n",
    "                    'labels': labels\n",
    "                })\n",
    "            print(f\"✅ 已加载被试 {subject_id} 的数据\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 加载失败 {npz_file}: {str(e)}\")\n",
    "\n",
    "    # === 第二步：按被试划分数据集 ===\n",
    "    all_subjects = sorted(subject_ids)  # 获取所有唯一被试ID\n",
    "    np.random.shuffle(all_subjects)      # 随机打乱\n",
    "    \n",
    "    test_subjs = all_subjects[:test_subjects]          # 前N个被试作为测试集\n",
    "    train_subjs = all_subjects[test_subjects:]         # 剩余作为训练集\n",
    "    \n",
    "    print(f\"\\n=== 数据集划分 ===\")\n",
    "    print(f\"总被试数: {len(all_subjects)}\")\n",
    "    print(f\"训练集被试 ({len(train_subjs)}): {', '.join(train_subjs)}\")\n",
    "    print(f\"测试集被试 ({len(test_subjs)}): {', '.join(test_subjs)}\")\n",
    "\n",
    "    # === 第三步：合并同一被试的所有样本 ===\n",
    "    def combine_subject_data(subjects):\n",
    "        \"\"\"合并指定被试的所有样本\"\"\"\n",
    "        samples, labels = [], []\n",
    "        for subj in subjects:\n",
    "            for data in subject_data[subj]:\n",
    "                samples.append(data['windows'])\n",
    "                labels.append(data['labels'])\n",
    "        return np.concatenate(samples), np.concatenate(labels)\n",
    "    \n",
    "    X_train, y_train = combine_subject_data(train_subjs)\n",
    "    X_test, y_test = combine_subject_data(test_subjs)\n",
    "\n",
    "    # === 第四步：保存TS文件 ===\n",
    "    def format_value(x):\n",
    "        \"\"\"保证6位小数\"\"\"\n",
    "        return f\"{float(x):.6f}\".replace(\"-0.000000\", \"0.000000\")\n",
    "\n",
    "    def save_ts_file(data, labels, file_suffix):\n",
    "        \"\"\"保存TS文件\"\"\"\n",
    "        ts_path = os.path.join(output_dir, f\"{dataset_name}_{file_suffix}.ts\")\n",
    "        error_count = 0\n",
    "        \n",
    "        with open(ts_path, 'w') as f:\n",
    "            # 文件头\n",
    "            f.write(f\"\"\"@problemName {dataset_name}\n",
    "@timestamps false\n",
    "@univariate false\n",
    "@equalLength true\n",
    "@dimensions {data.shape[1]}\n",
    "@seriesLength {data.shape[2]}\n",
    "@classLabel true 0 1\n",
    "@data\n",
    "\"\"\")\n",
    "            # 数据行\n",
    "            for i in range(len(data)):\n",
    "                try:\n",
    "                    # 每个通道的值用逗号分隔，通道间用冒号分隔\n",
    "                    channels = [\n",
    "                        \",\".join([format_value(x) for x in data[i, ch]]) \n",
    "                        for ch in range(data.shape[1])\n",
    "                    ]\n",
    "                    line = \":\".join(channels) + f\":{int(labels[i])}\"\n",
    "                    f.write(line + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    print(f\"样本{i}保存失败: {str(e)}\")\n",
    "        \n",
    "        return error_count\n",
    "\n",
    "    # 执行保存\n",
    "    print(\"\\n=== 保存文件中 ===\")\n",
    "    train_errors = save_ts_file(X_train, y_train, \"TRAIN\")\n",
    "    test_errors = save_ts_file(X_test, y_test, \"TEST\")\n",
    "\n",
    "    # === 第五步：保存元数据 ===\n",
    "    metadata_path = os.path.join(output_dir, f\"{dataset_name}_METADATA.txt\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(f\"\"\"# {dataset_name} 数据集元数据\n",
    "## 被试划分\n",
    "- 总被试数: {len(all_subjects)}\n",
    "- 训练集被试: {len(train_subjs)} ({', '.join(train_subjs)})\n",
    "- 测试集被试: {len(test_subjs)} ({', '.join(test_subjs)})\n",
    "\n",
    "## 数据统计\n",
    "- 训练样本数: {len(X_train)} (保存错误: {train_errors})\n",
    "- 测试样本数: {len(X_test)} (保存错误: {test_errors})\n",
    "- 通道数: {X_train.shape[1]}\n",
    "- 时间步长: {X_train.shape[2]}\n",
    "\n",
    "## 标签说明\n",
    "- 0: 非癫痫发作\n",
    "- 1: 癫痫发作\n",
    "\n",
    "生成时间: {np.datetime64('now')}\n",
    "\"\"\")\n",
    "\n",
    "    print(f\"\"\"\n",
    "=== 转换完成 ===\n",
    "输出文件:\n",
    "{output_dir}/\n",
    "├── {dataset_name}_TRAIN.ts\n",
    "├── {dataset_name}_TEST.ts\n",
    "└── {dataset_name}_METADATA.txt\n",
    "\n",
    "数据集统计:\n",
    "- 训练集: {len(X_train)} 样本 (来自 {len(train_subjs)} 个被试)\n",
    "- 测试集: {len(X_test)} 样本 (来自 {len(test_subjs)} 个被试)\n",
    "\"\"\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 输入文件列表（修改为你的实际路径）\n",
    "    input_files = [\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb19.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb02.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb23.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb01.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb03.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb21.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb20.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb18.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb14.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb13.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb10.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb09.npz',\n",
    "        '/root/autodl-tmp/balanced_data/balanced_chb05.npz'\n",
    "    ]\n",
    "    \n",
    "    # 输出目录（建议绝对路径）\n",
    "    output_dir = \"/root/autodl-tmp/Time-Series-Library/dataset\"\n",
    "    dataset_name = \"CHBMIT_EEG_BySubject\"\n",
    "    \n",
    "    # 执行转换（测试集使用3个被试）\n",
    "    convert_npz_to_ts_by_subject(\n",
    "        npz_files=[f for f in input_files if os.path.exists(f)],\n",
    "        output_dir=output_dir,\n",
    "        dataset_name=dataset_name,\n",
    "        test_subjects=3,  # 指定测试集被试数量\n",
    "        random_state=42\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
